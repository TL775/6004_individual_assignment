{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291b794-6a1c-4641-99a9-026cdbcd1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1803156-4f35-4515-98a5-0441cc0cd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sph6004_assignment1_data.csv')\n",
    "# One-Hot Encoding for the 'gender' column\n",
    "gender_encoded = pd.get_dummies(data['gender'], prefix='gender')\n",
    "\n",
    "# One-Hot Encoding for the 'race' column\n",
    "race_encoded = pd.get_dummies(data['race'], prefix='race')\n",
    "\n",
    "# Adding the One-Hot encoded results to the original DataFrame\n",
    "data_encoded = pd.concat([data, gender_encoded, race_encoded], axis=1)\n",
    "\n",
    "# Dropping the original 'gender' and 'race' columns\n",
    "data_encoded.drop(columns=['gender', 'race'], inplace=True)\n",
    "\n",
    "# Calculating the percentage of missing values for each variable\n",
    "missing_percentage = (data_encoded.isnull().sum() / len(data_encoded)) * 100\n",
    "\n",
    "# Identifying variables with missing value percentages exceeding 70%\n",
    "variables_to_drop = missing_percentage[missing_percentage > 70].index\n",
    "\n",
    "# Dropping variables with missing values exceeding 70%\n",
    "df = data_encoded.drop(variables_to_drop, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800f78b-8125-45a7-97d3-7bba8a747492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)  # The value of n_neighbors can be adjusted as needed\n",
    "\n",
    "# Filling in the missing values in the DataFrame\n",
    "df_filled = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6624ad-4207-4cbb-8ced-006cc7b6e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the data\n",
    "df_final = scaler.fit_transform(df_filled)\n",
    "\n",
    "# Converting the standardized data back to a DataFrame\n",
    "df_final = pd.DataFrame(df_final, columns=df_filled.columns)\n",
    "\n",
    "# Transforming the 'aki' column into binary format for binary classification\n",
    "df_final['aki'] = df_final['aki'].replace({0: 0, 1: 0, 2: 0, 3: 1})\n",
    "\n",
    "# Extracting features from the DataFrame\n",
    "df_feature = df_filled.iloc[:, 1:]\n",
    "\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3cbf7-5722-481f-a43e-3ecabcfce21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Initializing the RFE object, setting the logistic regression model and the number of features to select\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=30)\n",
    "\n",
    "# Selecting features\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Getting the indices of the selected features\n",
    "selected_features_indices = rfe.support_\n",
    "\n",
    "# Getting the selected features\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33240aa0-1829-4e06-b7bd-f1fee0ae87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', C=0.01)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Getting the selected features based on non-zero coefficients\n",
    "selected_features_1 = X.columns[model.coef_[0] != 0]\n",
    "selected_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4d5e3-083c-4836-8e29-95f4192097b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm\n",
    "df_sampled = df_feature.sample(n=3000, random_state=42)\n",
    "\n",
    "# Creating feature matrix X and target variable y\n",
    "X = df_sampled.drop(columns=['aki'])\n",
    "y = df_sampled['aki'].copy()\n",
    "\n",
    "# Defining the fitness function\n",
    "def evaluate(individual, X, y):\n",
    "    selected_features_0 = [bool(i) for i in individual]\n",
    "    X_selected = X.iloc[:, selected_features_0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred),\n",
    "\n",
    "# Defining genetic algorithm parameters\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.choice, [0, 1])\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(X.columns))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, X=X, y=y)\n",
    "toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Adjusting genetic algorithm parameters\n",
    "population_size = 100\n",
    "num_generations = 30  # Increasing the number of generations\n",
    "cxpb = 0.3  # Reducing crossover probability\n",
    "mutpb = 0.4  # Increasing mutation probability\n",
    "\n",
    "# Creating the population\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Running the genetic algorithm\n",
    "for generation in range(num_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=cxpb, mutpb=mutpb)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "# Getting the best individual\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [bool(i) for i in best_individual]\n",
    "selected_features_names = X.columns[selected_features]\n",
    "print(\"Selected features_2:\", selected_features_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f79551-1ee1-4e67-9287-a09bbbad2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Feature matrix\n",
    "X = sampled_df.drop(columns=['aki'])\n",
    "\n",
    "# Target variable\n",
    "y = sampled_df['aki']\n",
    "\n",
    "# Initializing the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Initializing the Sequential Feature Selector for forward feature selection\n",
    "selector = SequentialFeatureSelector(model, scoring='accuracy', cv=5)\n",
    "\n",
    "# Selecting features\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Getting the indices of the selected features\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Getting the selected features\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad03571-088d-4cf3-be95-e22fb522b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_2_heri = ['admission_age', 'sbp_max', 'dbp_min', 'mbp_min', 'mbp_max', 'mbp_mean',\n",
    "       'resp_rate_min', 'resp_rate_mean', 'spo2_mean', 'glucose_min',\n",
    "       'lactate_min', 'lactate_max', 'ph_min', 'ph_max', 'so2_min', 'so2_max',\n",
    "       'pco2_max', 'aado2_calc_min', 'pao2fio2ratio_min', 'baseexcess_max',\n",
    "       'totalco2_min', 'calcium_min', 'glucose_max.1', 'hematocrit_min.1',\n",
    "       'hemoglobin_max.1', 'platelets_max', 'albumin_min', 'aniongap_min',\n",
    "       'bicarbonate_min.1', 'bun_min', 'calcium_min.1', 'glucose_min.2',\n",
    "       'sodium_max.1', 'potassium_min.1', 'potassium_max.1',\n",
    "       'abs_basophils_min', 'abs_eosinophils_min', 'abs_lymphocytes_min',\n",
    "       'abs_lymphocytes_max', 'abs_monocytes_min', 'abs_neutrophils_max',\n",
    "       'inr_min', 'inr_max', 'pt_min', 'ptt_max', 'alp_max',\n",
    "       'bilirubin_total_max', 'ck_cpk_min', 'ck_cpk_max', 'gcs_eyes',\n",
    "       'weight_admit', 'gender_M', 'race_AMERICAN INDIAN/ALASKA NATIVE',\n",
    "       'race_ASIAN - ASIAN INDIAN', 'race_ASIAN - CHINESE',\n",
    "       'race_ASIAN - KOREAN', 'race_ASIAN - SOUTH EAST ASIAN',\n",
    "       'race_BLACK/AFRICAN', 'race_HISPANIC OR LATINO',\n",
    "       'race_HISPANIC/LATINO - CENTRAL AMERICAN',\n",
    "       'race_HISPANIC/LATINO - CUBAN', 'race_HISPANIC/LATINO - HONDURAN',\n",
    "       'race_HISPANIC/LATINO - MEXICAN', 'race_HISPANIC/LATINO - SALVADORAN',\n",
    "       'race_MULTIPLE RACE/ETHNICITY',\n",
    "       'race_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER',\n",
    "       'race_PATIENT DECLINED TO ANSWER', 'race_SOUTH AMERICAN',\n",
    "       'race_UNABLE TO OBTAIN', 'race_UNKNOWN', 'race_WHITE',\n",
    "       'race_WHITE - EASTERN EUROPEAN', 'race_WHITE - RUSSIAN']\n",
    "selected_features_3_genetic = ['admission_age', 'sbp_min', 'sbp_mean', 'dbp_min', 'dbp_max', 'mbp_min',\n",
    "       'mbp_max', 'mbp_mean', 'temperature_min', 'temperature_max',\n",
    "       'temperature_mean', 'spo2_max', 'glucose_min', 'ph_min', 'ph_max',\n",
    "       'so2_max', 'aado2_calc_min', 'calcium_max', 'glucose_min.1',\n",
    "       'glucose_max.1', 'potassium_min', 'platelets_max', 'albumin_max',\n",
    "       'glucose_min.2', 'glucose_max.2', 'sodium_min.1', 'potassium_min.1',\n",
    "       'abs_basophils_max', 'abs_eosinophils_min', 'abs_eosinophils_max',\n",
    "       'abs_neutrophils_max', 'inr_min', 'inr_max', 'pt_max', 'ast_min',\n",
    "       'gcs_min', 'gcs_verbal', 'gcs_eyes', 'weight_admit', 'gender_F',\n",
    "       'gender_M', 'race_AMERICAN INDIAN/ALASKA NATIVE', 'race_ASIAN',\n",
    "       'race_ASIAN - ASIAN INDIAN', 'race_ASIAN - CHINESE',\n",
    "       'race_ASIAN - KOREAN', 'race_ASIAN - SOUTH EAST ASIAN',\n",
    "       'race_BLACK/AFRICAN', 'race_BLACK/AFRICAN AMERICAN',\n",
    "       'race_BLACK/CAPE VERDEAN', 'race_BLACK/CARIBBEAN ISLAND',\n",
    "       'race_HISPANIC OR LATINO', 'race_HISPANIC/LATINO - CENTRAL AMERICAN',\n",
    "       'race_HISPANIC/LATINO - COLUMBIAN', 'race_HISPANIC/LATINO - CUBAN',\n",
    "       'race_HISPANIC/LATINO - DOMINICAN', 'race_HISPANIC/LATINO - GUATEMALAN',\n",
    "       'race_HISPANIC/LATINO - HONDURAN', 'race_HISPANIC/LATINO - MEXICAN',\n",
    "       'race_HISPANIC/LATINO - PUERTO RICAN',\n",
    "       'race_HISPANIC/LATINO - SALVADORAN', 'race_MULTIPLE RACE/ETHNICITY',\n",
    "       'race_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER', 'race_OTHER',\n",
    "       'race_PATIENT DECLINED TO ANSWER', 'race_PORTUGUESE',\n",
    "       'race_SOUTH AMERICAN', 'race_UNABLE TO OBTAIN', 'race_UNKNOWN',\n",
    "       'race_WHITE - BRAZILIAN', 'race_WHITE - EASTERN EUROPEAN']\n",
    "\n",
    "selected_features_1_L1 = ['admission_age', 'heart_rate_min', 'heart_rate_max', 'heart_rate_mean',\n",
    "       'sbp_min', 'sbp_max', 'sbp_mean', 'dbp_min', 'dbp_max', 'dbp_mean',\n",
    "       'mbp_min', 'mbp_max', 'mbp_mean', 'resp_rate_min', 'resp_rate_max',\n",
    "       'resp_rate_mean', 'temperature_min', 'spo2_min', 'spo2_max',\n",
    "       'spo2_mean', 'glucose_min', 'glucose_max', 'glucose_mean',\n",
    "       'lactate_min', 'so2_min', 'so2_max', 'po2_min', 'po2_max', 'pco2_min',\n",
    "       'pco2_max', 'aado2_calc_min', 'aado2_calc_max', 'pao2fio2ratio_min',\n",
    "       'pao2fio2ratio_max', 'baseexcess_min', 'baseexcess_max', 'totalco2_min',\n",
    "       'totalco2_max', 'glucose_min.1', 'glucose_max.1', 'potassium_max',\n",
    "       'hematocrit_min.1', 'hematocrit_max.1', 'hemoglobin_min.1',\n",
    "       'hemoglobin_max.1', 'platelets_min', 'platelets_max', 'wbc_min',\n",
    "       'wbc_max', 'albumin_min', 'albumin_max', 'aniongap_min', 'aniongap_max',\n",
    "       'bicarbonate_min.1', 'bicarbonate_max.1', 'bun_min', 'bun_max',\n",
    "       'calcium_min.1', 'calcium_max.1', 'chloride_max.1', 'glucose_min.2',\n",
    "       'glucose_max.2', 'sodium_min.1', 'sodium_max.1', 'potassium_max.1',\n",
    "       'abs_lymphocytes_min', 'abs_neutrophils_min', 'abs_neutrophils_max',\n",
    "       'pt_min', 'pt_max', 'ptt_min', 'ptt_max', 'alt_min', 'alt_max',\n",
    "       'alp_min', 'alp_max', 'ast_min', 'ast_max', 'bilirubin_total_max',\n",
    "       'ck_cpk_min', 'ck_cpk_max', 'gcs_min', 'gcs_motor', 'gcs_verbal',\n",
    "       'gcs_eyes', 'height', 'weight_admit', 'gender_M']\n",
    "\n",
    "selected_features_0_RFE = ['sbp_mean', 'dbp_min', 'dbp_mean', 'mbp_mean', 'resp_rate_mean',\n",
    "       'temperature_min', 'spo2_mean', 'lactate_min', 'hemoglobin_min.1',\n",
    "       'hemoglobin_max.1', 'albumin_min', 'albumin_max', 'aniongap_max',\n",
    "       'bicarbonate_min.1', 'bun_max', 'calcium_min.1', 'chloride_max.1',\n",
    "       'sodium_max.1', 'potassium_max.1', 'abs_neutrophils_min',\n",
    "       'abs_neutrophils_max', 'inr_max', 'bilirubin_total_min',\n",
    "       'bilirubin_total_max', 'gcs_verbal', 'gcs_eyes', 'gcs_unable', 'height',\n",
    "       'weight_admit', 'gender_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fa7e8-a3cd-408b-96e3-8e53dbb3ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature matrix X based on selected features\n",
    "X1 = data[selected_features_0_RFE]\n",
    "\n",
    "# Creating target variable y\n",
    "y1 = data['aki']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Training the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]  # Getting the probability of positive class prediction\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04242eaa-f868-4389-91cc-2db497b469d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Training the model on the training set\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = svm_model.decision_function(X_test)  # Getting the decision function values as prediction probabilities\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1737b-470f-414e-a611-d0573606517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the decision tree model\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Training the model on the training set\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = tree_model.predict_proba(X_test)[:, 1]  # Getting the probability of positive class prediction\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9739d0-b8d9-4261-9054-d0ca0520ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Training the model on the training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Getting the probability of positive class prediction\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fce64-8b28-466a-b8c4-fb00ec101f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "# Training the model on the training set\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = adaboost_model.predict_proba(X_test)[:, 1]  # Getting the probability of positive class prediction\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1855235-d472-41fa-805c-7f116c6fcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Gradient Boosting Tree model\n",
    "gbt_model = GradientBoostingClassifier()\n",
    "\n",
    "# Training the model on the training set\n",
    "gbt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = gbt_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting probabilities on the testing set\n",
    "y_pred_proba = gbt_model.predict_proba(X_test)[:, 1]  # Getting the probability of positive class prediction\n",
    "\n",
    "# Calculating ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Printing ROC-AUC\n",
    "print(\"ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a98ce-3f0f-47cc-be70-4b8fe6f7ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the above steps for the remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b0c2c-3e4d-4f9b-a606-a87f1ad6e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = df_feature.drop(columns=['aki'])\n",
    "\n",
    "# Target variable\n",
    "y = df_feature['aki']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Use decision tree-based feature selection method\n",
    "selector = SelectFromModel(estimator=clf)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_features_indices = selector.get_support()\n",
    "\n",
    "# Filter features based on the selected feature indices\n",
    "X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on the test set:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8bc40-9639-42a8-90b0-b5e0b06a7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = df_feature.drop(columns=['aki'])\n",
    "\n",
    "# Target variable\n",
    "y = df_feature['aki']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the random forest model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Use random forest-based feature selection method\n",
    "selector = SelectFromModel(estimator=clf)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_features_indices = selector.get_support()\n",
    "\n",
    "# Filter features based on the selected feature indices\n",
    "X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on the test set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613ce4d-e4fc-4206-a768-6aefa8db9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = df_feature.drop(columns=['aki'])\n",
    "\n",
    "# Target variable\n",
    "y = df_feature['aki']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the AdaBoost model\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "# Use AdaBoost-based feature selection method\n",
    "selector = SelectFromModel(estimator=clf)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_features_indices = selector.get_support()\n",
    "\n",
    "# Filter features based on the selected feature indices\n",
    "X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on the test set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78830481-4586-409f-8a9c-bcc99e1c3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = df_feature.drop(columns=['aki'])\n",
    "\n",
    "# Target variable\n",
    "y = df_feature['aki']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Tree model\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Use Gradient Boosting Tree-based feature selection method\n",
    "selector = SelectFromModel(estimator=clf)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_features_indices = selector.get_support()\n",
    "\n",
    "# Filter features based on the selected feature indices\n",
    "X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on the test set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64744d1a-9197-4394-908f-68edfa96fa51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
